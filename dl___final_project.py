# -*- coding: utf-8 -*-
"""DL | Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKtVkTXg6AgL41KQ_pLFsqikad6q3-EC
"""

from google.colab import drive
drive.mount('/content/drive')

# Libraries
import os
import json
import torch
import time
import torch.nn as nn
import string
import torchvision.transforms as T
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image, ImageDraw, ImageFont, ImageOps

import kagglehub

# Download latest version
path = kagglehub.dataset_download("robikscube/textocr-text-extraction-from-images-dataset")

print("Path to dataset files:", path)

# Checking the contents of the file
print(os.listdir(path))

if path == "/kaggle/input/textocr-text-extraction-from-images-dataset":
  path = "/kaggle/input/textocr-text-extraction-from-images-dataset/train_val_images"
  print(os.listdir(path))
elif path == "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2":
  path = "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2/train_val_images"
  print(os.listdir(path))

# Annotation Tables
# df = pd.read_csv("/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2/annot.csv")
# df.head()

df = pd.read_csv("/kaggle/input/textocr-text-extraction-from-images-dataset/annot.csv")
df.head()

base = "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2"

# Load JSON
with open(os.path.join(base, "TextOCR_0.1_train.json"), "r") as f:
  data = json.load(f)

# Check first image entry
img_id = list(data["imgs"].keys())[0]
img_info = data["imgs"][img_id]

print("Image ID:", img_id)
print(img_info)

# Clean filename (remove "train/")
# file_name = img_info["file_name"].split("/")[-1]

# Show the actual image
# img_path = os.path.join(base, "train_val_images", "train_images", file_name)
# img = Image.open(img_path)
# img

# Extract all annotations for THIS image
ann_list = []

for ann_id, ann in data["anns"].items():
  if ann["image_id"] == img_id:
    text = ann["utf8_string"]
    points = ann["points"]
    ann_list.append((text, points))


print("Found", len(ann_list), "words in this image")
ann_list[:5]

ann_path = "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2/TextOCR_0.1_train.json"
with open(ann_path, "r") as f:
    data = json.load(f)

print(type(data))
print(data.keys())

print(type(data["imgs"]))
print(len(data["imgs"]))
# Show the first 3 entries
first_three = list(data["imgs"].items())[:3] if isinstance(data["imgs"], dict) else data["imgs"][:3]
print(first_three)

# Visualizing Bounding Boxes from the TextOCR dataset (Just 1 image)

# Load the annotation JSON
# ann_path = "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2/TextOCR_0.1_train.json"
ann_path = "/kaggle/input/textocr-text-extraction-from-images-dataset/TextOCR_0.1_train.json"
with open(ann_path, "r") as f:
  data = json.load(f)

# # Helper: find annotation for a given image id
# def get_annotation(image_id):
#   return data["imgs"].get(image_id, None)

# Choosing example image
image_id = "a4ea732cd3d5948a"
img_info = data["imgs"][image_id]

# Load all annotations for this image
ann_ids = data["imgToAnns"][image_id]
words = [data["anns"][str(aid)] for aid in ann_ids]

print(f"Found {len(words)} words")
print(words[:3])

# img_info = get_annotation(image_id)
# print("Image info:", img_info)

# Load the actual image file
# base = "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2/train_val_images/train_images"
base = "/kaggle/input/textocr-text-extraction-from-images-dataset/train_val_images/train_images"

filename = os.path.basename(img_info["file_name"])
img_path = os.path.join(base, filename)
img = Image.open(img_path).convert("RGB")

draw = ImageDraw.Draw(img)

# Draw polygons and text labels
for word in words:
  label = word["utf8_string"]
  poly = word["points"] # List of 8 floats

  # Rearrange into (x,y) pairs
  xy = [(poly[i], poly[i+1]) for i in range(0, 8, 2)]

  # Draw polygon
  draw.polygon(xy, outline="yellow")
  draw.text(xy[0], label, fill="yellow")

# Show the result
plt.figure(figsize=(10,12))
plt.imshow(img)
plt.axis("off")
plt.show()

# Function for making a rectangles (Bounding Boxes)
def polygon_to_bbox(x1,y1,x2,y2,w,h):
  x1 = max(0, min(x1, w-1))
  x2 = max(0, min(x2, w-1))
  y1 = max(0, min(y1, h-1))
  y2 = max(0, min(y2, h-1))

  # Fix reversed vals
  if x2 < x1:
    x1, x2 = x2, x1
  if y2 < y1:
    y1, y2 = y2, y1

  return x1,y1,x2,y2

# Function for Cropping the image
# Load image -> get annotations -> convert polygons to bounding box -> crop the word
def crop_words(image_id, data, img_base_path):
  img_info = data["imgs"][image_id]

  # Load image
  filename = os.path.basename(img_info["file_name"])
  img_path = os.path.join(img_base_path, filename)
  img = Image.open(img_path).convert("RGB")

  # Get annotation IDs for the image
  ann_ids = data["imgToAnns"][image_id]
  W, H = img.size

  crops = [] # (cropped_image, label)
  for ann_id in ann_ids:
    ann = data["anns"][str(ann_id)]

    text = ann["utf8_string"] # Label
    points = ann["points"] # Polygons

    xs = points[0::2]
    ys = points[1::2]

    # Convert polygon -> bbox
    x1, y1, x2, y2 = polygon_to_bbox(min(xs), min(ys), max(xs), max(ys), W, H)

    # Skip tiny boxes
    if (x2 - x1) < 2 or (y2 - y1) < 2:
      continue

    # Crop
    crop = img.crop((x1,y1,x2,y2))

    crops.append((crop, text))
  return crops

# Load the annotation JSON
# ann_path = "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2/TextOCR_0.1_train.json"
ann_path = "/kaggle/input/textocr-text-extraction-from-images-dataset/TextOCR_0.1_train.json"
with open(ann_path, "r") as f:
  data = json.load(f)

# Testing the functions
image_id = "a4ea732cd3d5948a"
# img_base = "/root/.cache/kagglehub/datasets/robikscube/textocr-text-extraction-from-images-dataset/versions/2/train_val_images/train_images"
img_base = "/kaggle/input/textocr-text-extraction-from-images-dataset/train_val_images/train_images"


word_crops = crop_words(image_id, data, img_base)

print("Total cropped words:", len(word_crops))

# Show first 5
for i in range(5):
  crop, txt = word_crops[i]
  print(f"Word {i}: '{txt}' Size: {crop.size}")
  display(crop)

# Preprocessing the dataset
# Desired OCR input size
IMG_W = 128
IMG_H = 32


# Text-to-index mapping
# Basic charset: A-Z, a-z, digits, punctuation
ALPHABET = string.ascii_letters + string.digits + string.punctuation + " "
UNK_IDX = len(ALPHABET) + 1


# CTC blank token at index 0
CHAR2IDX = {c: i+1 for i, c in enumerate(ALPHABET)}
IDX2CHAR = {i+1: c for i, c in enumerate(ALPHABET)}


# Encode/decode text
def encode_text(text):
  out = []
  for ch in text:
    out.append(CHAR2IDX.get(ch, UNK_IDX))
  return out

def decode_seq(seq):
  return ''.join([IDX2CHAR.get(i, '') for i in seq])

transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.Resize((IMG_H, IMG_W)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5)) # Normalize grayscale to [-1,1]
])


# Making the PyTorch Dataset
class OCRDataset(Dataset):
  def __init__(self, crops):
    self.crops = [(c["crop"], c["text"]) if isinstance(c, dict) else c for c in crops]
    self.transform = transform

  def __len__(self):
    return len(self.crops)

  def __getitem__(self, idx):
    img_pil, text = self.crops[idx]
    img = self.transform(img_pil)
    label = torch.tensor(encode_text(text), dtype=torch.long)

    return img, label

def ocr_collate(batch):
  imgs = []
  labels = []
  label_lengths = []

  for img, label in batch:
    imgs.append(img)
    labels.extend(label.tolist())
    label_lengths.append(len(label))

  imgs = torch.stack(imgs)
  labels = torch.tensor(labels, dtype=torch.long)
  label_lengths = torch.tensor(label_lengths, dtype=torch.long)

  return imgs, labels, label_lengths

# Create a DatasetLoader
dataset = OCRDataset(word_crops)
loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=ocr_collate, num_workers=2, pin_memory=True)

# Build the CRNN model
class CRNN(nn.Module):
  def __init__(self, num_classes):
    super().__init__()

    self.cnn = nn.Sequential(
        nn.Conv2d(1, 64, 3, 1, 1),
        nn.ReLU(),
        nn.MaxPool2d(2,2,), # H:32->16

        nn.Conv2d(64,128,3,1,1),
        nn.ReLU(),
        nn.MaxPool2d(2,2), # H:16->8

        nn.Conv2d(128,256,3,1,1),
        nn.ReLU(),

        # Reduce height to 1 (time axis = width only)
        nn.MaxPool2d(kernel_size=(8,1)) # H:8->1
    )

    self.rnn = nn.LSTM(
        input_size=256,
        hidden_size=256,
        num_layers=2,
        bidirectional=True,
        batch_first=True
    )

    self.fc = nn.Linear(512, num_classes)

  def forward(self, x):
    x = self.cnn(x) # [B, C=256, H=1, W=time]
    B, C, H, W = x.size()
    x = x.permute(0, 3, 1, 2).reshape(B, W, C) # [B, T=W, C]
    x, _ = self.rnn(x) # [B, T, 512]
    x = self.fc(x) # [B, T, num_classes]
    return x

# Classes count = alphabet length + CTC blank:
num_classes = len(ALPHABET) + 2 # +1 for UNK, +1 reserved blank (0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

model = CRNN(num_classes).to(device)
criterion = nn.CTCLoss(blank=0)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

def train_epoch(loader):
  model.train()
  epoch_loss = 0.0
  total_loss = 0
  n_batches = 0
  t0 = time.time()

  for imgs, labels, label_lengths in loader:
    imgs = imgs.to(device, non_blocking=True) # [B, 1, H, W]
    labels = labels.to(device) # flattened 1D tensor
    label_lengths = label_lengths.to(device)

    preds = model(imgs) # [B, T, C]
    preds = preds.permute(1,0,2) # [T, B, C] for CTCLoss

    pred_lenghts = torch.full(
        size=(imgs.size(0),),
        fill_value = preds.size(0),
        dtype = torch.long,
        device = device
    )

    loss = criterion(
        preds.log_softmax(2), labels,
        pred_lenghts, label_lengths
    )

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    epoch_loss += loss.item()
    n_batches += 1

  t1 = time.time()
  print(f"Train loss: {epoch_loss:.4f} | batches: {n_batches} | time: {t1-t0:.1f}s")
  print("Loss:", total_loss)

EPOCHS = 50
for ep in range(EPOCHS):
  print(f"EPOCH {ep+1}/{EPOCHS} === ")
  train_epoch(loader)

imgs, labels, label_lengths = next(iter(loader))
imgs = imgs.to(device)
labels = labels.to(device)

with torch.no_grad():
    preds = model(imgs).permute(1, 0, 2)
    print("Predicted time steps:", preds.size(0))
    print("Label lengths (first 10):", label_lengths[:10].tolist())

    # check if logits are insane
    print("Pred min/max logits:", preds.min().item(), preds.max().item())